{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Data Loading and Preprocessing\n",
    "import pandas as pd\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\" \n",
    "import warnings\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "# Suppress UndefinedMetricWarning\n",
    "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "class VulnerabilityDataLoader:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        \n",
    "    def load_data(self, language):\n",
    "        \"\"\"Load data for specific language\"\"\"\n",
    "        file_path = self.config['paths'][f'sample_{language}.csv']\n",
    "        \n",
    "        # Try different encodings until one works\n",
    "        encodings = ['utf-8', 'latin1', 'iso-8859-1', 'cp1252']\n",
    "\n",
    "        for encoding in encodings:\n",
    "            try:\n",
    "                df = pd.read_csv(file_path, encoding=encoding)\n",
    "                break\n",
    "            except UnicodeDecodeError:\n",
    "                continue\n",
    "\n",
    "        if language == 'Python':\n",
    "            df = pd.read_csv(file_path)\n",
    "            df = df.rename(columns={'code': 'file_path'})\n",
    "            # Python specific processing\n",
    "        else:\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "        # Standardize column names\n",
    "        if 'line_no' not in df.columns:\n",
    "            df['line_no'] = -1\n",
    "            \n",
    "        print(\"Loaded data for language:\", language)\n",
    "        return df\n",
    "    \n",
    "    def get_source_code(self, file_path, language):\n",
    "        \"\"\"Retrieve source code from file\"\"\"\n",
    "        if language == 'Python':\n",
    "            return file_path\n",
    "        base_path = self.config['source_code_paths'][f'{language}SourceCodes']\n",
    "        full_path = os.path.join(base_path, file_path)\n",
    "        \n",
    "        try:\n",
    "            with open(full_path, 'r') as f:\n",
    "                return f.read()\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Warning: File not found {full_path}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Structural Feature Extractor\n",
    "import subprocess\n",
    "import json\n",
    "import tempfile\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "class StructuralFeatureExtractor:\n",
    "    def __init__(self):\n",
    "        # Semgrep configuration\n",
    "        self.semgrep_config = {\n",
    "            'c': 'p/c',\n",
    "            'cpp': 'p/c',\n",
    "            'java': 'p/java',\n",
    "            'python': 'p/python',\n",
    "            'csharp': 'p/csharp'\n",
    "        }\n",
    "        self.language_mapping = {\n",
    "            'CandCpp': 'c',\n",
    "            'Java': 'java',\n",
    "            'Python': 'python',\n",
    "            'CSharp': 'csharp'\n",
    "        }\n",
    "        \n",
    "        # Initialize CodeBERT with eager attention\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "        self.model = AutoModel.from_pretrained(\"microsoft/codebert-base\",\n",
    "                                            output_attentions=True,\n",
    "                                            attn_implementation=\"eager\")\n",
    "        \n",
    "        # Default feature template\n",
    "        self.default_features = {\n",
    "            'semgrep_info': 0,\n",
    "            'semgrep_warning': 0,\n",
    "            'semgrep_error': 0,\n",
    "            'semgrep_total_findings': 0,\n",
    "            'semgrep_rules_triggered': 0,\n",
    "            'line_no': -1\n",
    "        }\n",
    "\n",
    "    def _get_important_lines(self, code):\n",
    "        \"\"\"Improved line detection using attention and AST parsing\"\"\"\n",
    "        try:\n",
    "            if not code.strip():\n",
    "                return []\n",
    "\n",
    "            inputs = self.tokenizer(code, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "            \n",
    "            # Create accurate line number mapping\n",
    "            line_numbers = []\n",
    "            current_line = 1\n",
    "            char_pos = 0\n",
    "            \n",
    "            for token_id in inputs.input_ids[0]:\n",
    "                token = self.tokenizer.decode(token_id)\n",
    "                if token in [self.tokenizer.cls_token, self.tokenizer.sep_token]:\n",
    "                    line_numbers.append(-1)\n",
    "                    continue\n",
    "                    \n",
    "                # Find the token in the original code\n",
    "                token_pos = code.find(token, char_pos)\n",
    "                if token_pos == -1:\n",
    "                    line_numbers.append(-1)\n",
    "                    continue\n",
    "                    \n",
    "                # Count newlines up to this position\n",
    "                current_line = code.count('\\n', 0, token_pos) + 1\n",
    "                line_numbers.append(current_line)\n",
    "                char_pos = token_pos + len(token)\n",
    "\n",
    "            # Get attention weights\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs)\n",
    "            \n",
    "            # Use attention from last layer\n",
    "            cls_attention = outputs.attentions[-1][:, :, 0, :].mean(dim=1)[0]\n",
    "            topk = min(5, len(cls_attention))\n",
    "            important_indices = torch.topk(cls_attention, k=topk).indices.tolist()\n",
    "\n",
    "            # Get unique line numbers and filter invalid ones\n",
    "            important_lines = sorted({\n",
    "                line_numbers[i] \n",
    "                for i in important_indices \n",
    "                if i < len(line_numbers) and line_numbers[i] != -1\n",
    "            })\n",
    "            \n",
    "            return important_lines[:3]  # Return top 3 most important lines\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[WARNING] Line detection failed: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "    def _run_semgrep_scan(self, code, language):\n",
    "        \"\"\"Execute Semgrep scan safely\"\"\"\n",
    "        semgrep_lang = self.language_mapping.get(language)\n",
    "        if semgrep_lang is None:\n",
    "            return None\n",
    "            \n",
    "        try:\n",
    "            with tempfile.NamedTemporaryFile(suffix=f'.{semgrep_lang}', mode='w') as tmp:\n",
    "                tmp.write(code)\n",
    "                tmp.flush()\n",
    "                cmd = f\"semgrep --config={self.semgrep_config[semgrep_lang]} --json {tmp.name}\"\n",
    "                result = subprocess.run(cmd, shell=True, capture_output=True, text=True, timeout=30)\n",
    "                return json.loads(result.stdout) if result.returncode == 0 else None\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Semgrep failed: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def extract_features(self, code, language):\n",
    "        \"\"\"Returns flat dictionary of features\"\"\"\n",
    "        features = self.default_features.copy()\n",
    "        if not code or not isinstance(code, str):\n",
    "            return features\n",
    "\n",
    "        # 1. Run Semgrep scan\n",
    "        semgrep_results = self._run_semgrep_scan(code, language)\n",
    "        if semgrep_results:\n",
    "            features.update(self._flatten_semgrep_results(semgrep_results))\n",
    "        \n",
    "        # 2. Get important lines (improved version)\n",
    "        important_lines = self._get_important_lines(code)\n",
    "        if important_lines:\n",
    "            features['line_no'] = important_lines[0]  # Most important line\n",
    "        \n",
    "        # 3. Add semantic and lexical features\n",
    "        semantic = self._get_semantic_features(code)\n",
    "        lexical = self._get_lexical_features(code)\n",
    "        \n",
    "        features.update({\n",
    "            f'semantic_{i}': val for i, val in enumerate(semantic)\n",
    "        })\n",
    "        features.update({\n",
    "            f'lexical_{i}': val for i, val in enumerate(lexical)\n",
    "        })\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def _get_semantic_features(self, code):\n",
    "        \"\"\"Get CodeBERT embeddings\"\"\"\n",
    "        try:\n",
    "            inputs = self.tokenizer(code, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs)\n",
    "            return outputs.last_hidden_state.mean(dim=1).numpy().flatten()\n",
    "        except:\n",
    "            return np.zeros(768)\n",
    "\n",
    "    def _get_lexical_features(self, code):\n",
    "        \"\"\"Get FastText embeddings\"\"\"\n",
    "        return np.zeros(300)  # Placeholder - implement actual FastText extraction\n",
    "    \n",
    "    def _flatten_semgrep_results(self, results):\n",
    "        \"\"\"Convert Semgrep output to features\"\"\"\n",
    "        if not results or 'results' not in results:\n",
    "            return {}\n",
    "            \n",
    "        features = {}\n",
    "        severities = {'INFO': 0, 'WARNING': 0, 'ERROR': 0}\n",
    "        \n",
    "        for finding in results.get('results', []):\n",
    "            severity = finding.get('extra', {}).get('severity', 'INFO')\n",
    "            severities[severity] += 1\n",
    "            \n",
    "        features.update({\n",
    "            f'semgrep_{severity.lower()}': count \n",
    "            for severity, count in severities.items()\n",
    "        })\n",
    "        \n",
    "        features['semgrep_total_findings'] = len(results.get('results', []))\n",
    "        features['semgrep_rules_triggered'] = len(set(\n",
    "            f.get('check_id', '') for f in results.get('results', [])\n",
    "        ))\n",
    "        \n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Semantic Context (CodeBERT)\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "class SemanticFeatureExtractor:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "        self.model = AutoModel.from_pretrained(\"microsoft/codebert-base\")\n",
    "        \n",
    "    def extract_features(self, code):\n",
    "        \"\"\"Extract semantic features using CodeBERT\"\"\"\n",
    "        inputs = self.tokenizer(code, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "        return outputs.last_hidden_state.mean(dim=1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Lexical Context (FastText)\n",
    "import fasttext\n",
    "import fasttext.util\n",
    "import numpy as np\n",
    "\n",
    "class LexicalFeatureExtractor:\n",
    "    def __init__(self):\n",
    "        fasttext.util.download_model('en', if_exists='ignore')\n",
    "        self.model = fasttext.load_model('cc.en.300.bin')\n",
    "        \n",
    "    def extract_features(self, code):\n",
    "        \"\"\"Extract lexical features using FastText\"\"\"\n",
    "        tokens = code.split()  # Simple tokenization\n",
    "        vectors = [self.model.get_word_vector(token) for token in tokens if token]\n",
    "        if vectors:\n",
    "            return np.mean(vectors, axis=0)\n",
    "        else:\n",
    "            return np.zeros(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Model Architecture - Classification Models\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "class ClassificationModels:\n",
    "    def __init__(self):\n",
    "        # Define parameter grids for hyperparameter tuning\n",
    "        self.param_grids = {\n",
    "            'random_forest': {\n",
    "                'n_estimators': [100, 200],\n",
    "                'max_depth': [None, 10, 20],\n",
    "                'min_samples_split': [2, 5]\n",
    "            },\n",
    "            'xgboost': {\n",
    "                'n_estimators': [100, 200],\n",
    "                'max_depth': [3, 6, 9],\n",
    "                'learning_rate': [0.01, 0.1]\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Base models\n",
    "        self.base_models = {\n",
    "            'random_forest': RandomForestClassifier(),\n",
    "            'knn': KNeighborsClassifier(),\n",
    "            'svm': SVC(probability=True),\n",
    "            'naive_bayes': GaussianNB(),\n",
    "            'xgboost': XGBClassifier()\n",
    "        }\n",
    "        \n",
    "    def train(self, X_train, y_train):\n",
    "        \"\"\"Train all classification models with hyperparameter tuning\"\"\"\n",
    "        print(\"Training Classification models with hyperparameter tuning...\")\n",
    "        trained_models = {}\n",
    "        \n",
    "        # Train models with hyperparameter tuning\n",
    "        for name, model in self.base_models.items():\n",
    "            if name in self.param_grids:\n",
    "                # Perform grid search for models with defined parameter grids\n",
    "                grid_search = GridSearchCV(\n",
    "                    estimator=model,\n",
    "                    param_grid=self.param_grids[name],\n",
    "                    cv=3,\n",
    "                    n_jobs=-1,\n",
    "                    verbose=1\n",
    "                )\n",
    "                grid_search.fit(X_train, y_train)\n",
    "                trained_models[name] = grid_search.best_estimator_\n",
    "                print(f\"Best params for {name}: {grid_search.best_params_}\")\n",
    "            else:\n",
    "                # Train without hyperparameter tuning\n",
    "                model.fit(X_train, y_train)\n",
    "                trained_models[name] = model\n",
    "                \n",
    "        print(\"Training classification models completed...\")\n",
    "        return trained_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated GraphCNNModels class with working GCN and GAT\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, MultiHeadAttention, LayerNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Reshape, GlobalAveragePooling1D\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow as tf\n",
    "from spektral.layers import GCNConv, GATConv, GlobalSumPool\n",
    "from scipy.sparse import eye as sparse_eye\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from spektral.utils import normalized_adjacency\n",
    "from spektral.layers import GCNConv, GATConv\n",
    "from spektral.utils import normalized_adjacency\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from spektral.layers import GCNConv, GATConv, GlobalSumPool\n",
    "from spektral.utils import normalized_adjacency\n",
    "from scipy.sparse import eye\n",
    "import numpy as np\n",
    "\n",
    "class GraphCNNModels:\n",
    "    def __init__(self, input_shape):\n",
    "        self.input_shape = input_shape if isinstance(input_shape, tuple) else (input_shape,)\n",
    "        \n",
    "    def build_attention_model(self):\n",
    "        \"\"\"Improved transformer-based model with better architecture\"\"\"\n",
    "        inputs = Input(shape=self.input_shape)\n",
    "        \n",
    "        # Reshape input to 3D for attention (batch, seq_len, features)\n",
    "        x = Reshape((1, self.input_shape[0]))(inputs)\n",
    "        \n",
    "        # Multi-head attention with skip connections\n",
    "        attn_output = MultiHeadAttention(num_heads=8, key_dim=64)(x, x)\n",
    "        x = LayerNormalization(epsilon=1e-6)(x + attn_output)\n",
    "        \n",
    "        # Feed forward network\n",
    "        x = Dense(512, activation='relu')(x)\n",
    "        x = Dropout(0.4)(x)\n",
    "        x = Dense(256, activation='relu')(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "        \n",
    "        # Global average pooling\n",
    "        x = GlobalAveragePooling1D()(x)\n",
    "        \n",
    "        # Final classification layers\n",
    "        x = Dense(128, activation='relu')(x)\n",
    "        x = Dropout(0.2)(x)\n",
    "        outputs = Dense(4, activation='softmax')(x)\n",
    "        \n",
    "        return Model(inputs, outputs)\n",
    "    \n",
    "    def build_graphsage_model(self):\n",
    "        \"\"\"Simplified GraphSAGE model\"\"\"\n",
    "        inputs = Input(shape=self.input_shape)\n",
    "        \n",
    "        # Feature transformation\n",
    "        x = Dense(512, activation='relu')(inputs)\n",
    "        x = Dropout(0.4)(x)\n",
    "        \n",
    "        # Node embedding\n",
    "        x = Dense(256, activation='relu')(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "        \n",
    "        # Classification\n",
    "        x = Dense(128, activation='relu')(x)\n",
    "        outputs = Dense(4, activation='softmax')(x)\n",
    "        \n",
    "        return Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    def build_simple_gnn(self):\n",
    "        \"\"\"Very simple graph neural network\"\"\"\n",
    "        inputs = Input(shape=self.input_shape)\n",
    "        \n",
    "        x = Dense(512, activation='relu')(inputs)\n",
    "        x = Dropout(0.4)(x)\n",
    "        \n",
    "        x = Dense(256, activation='relu')(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "        \n",
    "        x = Dense(128, activation='relu')(x)\n",
    "        outputs = Dense(4, activation='softmax')(x)\n",
    "        \n",
    "        return Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    def train(self, X_train, y_train, epochs=20, batch_size=32):\n",
    "        trained_models = {}\n",
    "        \n",
    "        # Convert labels to one-hot\n",
    "        y_train_onehot = tf.keras.utils.to_categorical(y_train, num_classes=4)\n",
    "        \n",
    "        # Normalize input data\n",
    "        X_train = X_train.astype('float32')\n",
    "        self.scaler = StandardScaler()\n",
    "        X_train_scaled = self.scaler.fit_transform(X_train)\n",
    "        \n",
    "        # Build models\n",
    "        models = {\n",
    "            'attention_model': self.build_attention_model(),\n",
    "            'graphsage_model': self.build_graphsage_model(),\n",
    "            'simple_gnn': self.build_simple_gnn()\n",
    "        }\n",
    "        \n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "        \n",
    "        for name, model in models.items():\n",
    "            print(f\"\\nTraining {name}...\")\n",
    "            model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "                         loss='categorical_crossentropy',\n",
    "                         metrics=['accuracy'])\n",
    "            \n",
    "            model.fit(X_train_scaled, y_train_onehot,\n",
    "                     validation_split=0.2,\n",
    "                     epochs=epochs, \n",
    "                     batch_size=batch_size,\n",
    "                     callbacks=[early_stopping],\n",
    "                     verbose=1)\n",
    "            \n",
    "            trained_models[name] = model\n",
    "        \n",
    "        return trained_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Ensemble Model\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "import numpy as np\n",
    "\n",
    "# Updated EnsembleModel class\n",
    "class EnsembleModel:\n",
    "    def __init__(self, classification_models, cnn_models):\n",
    "        self.classification_models = classification_models\n",
    "        self.cnn_models = cnn_models\n",
    "        self.n_features = None\n",
    "        self.adj_matrix = None\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions using weighted ensemble of all models\"\"\"\n",
    "        if self.n_features is None:\n",
    "            self.n_features = X.shape[1]\n",
    "            # Create adjacency matrix once\n",
    "            from scipy.sparse import eye as sparse_eye\n",
    "            adj_sparse = sparse_eye(self.n_features, format='coo')\n",
    "            self.adj_matrix = adj_sparse.toarray()\n",
    "        \n",
    "        all_predictions = []\n",
    "        model_weights = {\n",
    "            'random_forest': 0.2,\n",
    "            'xgboost': 0.2,\n",
    "            'attention_model': 0.2,\n",
    "            'gcn_model': 0.2,\n",
    "            'gat_model': 0.2\n",
    "        }\n",
    "        \n",
    "        # Create adjacency matrices for graph models\n",
    "        adj_matrices = np.array([self.adj_matrix for _ in range(len(X))])\n",
    "        \n",
    "        # Get predictions from classification models\n",
    "        for name, model in self.classification_models.items():\n",
    "            if name in model_weights:\n",
    "                if hasattr(model, 'predict_proba'):\n",
    "                    preds = model.predict_proba(X) * model_weights[name]\n",
    "                else:\n",
    "                    preds = model.decision_function(X)\n",
    "                    # Normalize to probabilities\n",
    "                    preds = (preds - preds.min()) / (preds.max() - preds.min()) * model_weights[name]\n",
    "                all_predictions.append(preds)\n",
    "        \n",
    "        # Get predictions from CNN models\n",
    "        for name, model in self.cnn_models.items():\n",
    "            if name in model_weights:\n",
    "                if name in ['gcn_model', 'gat_model']:\n",
    "                    # For graph models, need to provide adjacency matrix\n",
    "                    cnn_preds = model.predict([X, adj_matrices]) * model_weights[name]\n",
    "                else:\n",
    "                    cnn_preds = model.predict(X) * model_weights[name]\n",
    "                all_predictions.append(cnn_preds)\n",
    "        \n",
    "        # Weighted average all predictions\n",
    "        if all_predictions:\n",
    "            avg_preds = np.sum(all_predictions, axis=0)\n",
    "            final_preds = np.argmax(avg_preds, axis=1)\n",
    "        else:\n",
    "            final_preds = np.zeros(X.shape[0])\n",
    "            \n",
    "        return final_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Evaluation Framework\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "\n",
    "class Evaluator:\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
    "            X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "        print(\"Training set counts: \", Counter(self.y_train))\n",
    "        print(\"Testing set counts: \", Counter(self.y_test))\n",
    "        self.ensemble = None \n",
    "        \n",
    "    def evaluate_models(self):\n",
    "        \"\"\"Evaluate all individual models and the ensemble\"\"\"\n",
    "        # Train classification models\n",
    "        clf = ClassificationModels()\n",
    "        trained_clf_models = clf.train(self.X_train, self.y_train)\n",
    "        print(\"Training classification models completed...\")\n",
    "        \n",
    "        # Print input shape for debugging\n",
    "        print(f\"Input shape for CNN models: {self.X_train.shape}\")\n",
    "\n",
    "        # Train CNN models\n",
    "        cnn = GraphCNNModels(input_shape=(self.X_train.shape[1],))\n",
    "        trained_cnn_models = cnn.train(self.X_train, self.y_train)\n",
    "        print(\"Training Graph CNN models completed...\")\n",
    "        \n",
    "        # Create ensemble\n",
    "        print(\"Creating Ensemble model...\")\n",
    "        self.ensemble = EnsembleModel(trained_clf_models, trained_cnn_models)\n",
    "        print(\"Ensembling Model Done...\")\n",
    "        \n",
    "        # Evaluate individual models\n",
    "        results = {}\n",
    "        for name, model in trained_clf_models.items():\n",
    "            y_pred = model.predict(self.X_test)\n",
    "            results[name] = self._calculate_metrics(self.y_test, y_pred)\n",
    "            \n",
    "        for name, model in trained_cnn_models.items():\n",
    "            y_pred = np.argmax(model.predict(self.X_test), axis=1)\n",
    "            results[name] = self._calculate_metrics(self.y_test, y_pred)\n",
    "            \n",
    "        # Evaluate ensemble\n",
    "        y_pred_ensemble = self.ensemble.predict(self.X_test)\n",
    "        results['ensemble'] = self._calculate_metrics(self.y_test, y_pred_ensemble)\n",
    "        print(\"Got Results...\")\n",
    "        return results\n",
    "    \n",
    "    def _calculate_metrics(self, y_true, y_pred):\n",
    "        \"\"\"Calculate evaluation metrics\"\"\"\n",
    "        return {\n",
    "            'accuracy': accuracy_score(y_true, y_pred),\n",
    "            'precision': precision_score(y_true, y_pred, average='weighted'),\n",
    "            'recall': recall_score(y_true, y_pred, average='weighted'),\n",
    "            'f1': f1_score(y_true, y_pred, average='weighted')\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Feature Vectorizer\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "class FeatureVectorizer:\n",
    "    def __init__(self):\n",
    "        self.vectorizer = DictVectorizer(sparse=False)\n",
    "        self.semantic_shape = None\n",
    "        self.lexical_shape = None\n",
    "        \n",
    "    def fit_transform(self, features_list):\n",
    "        \"\"\"Convert list of feature dicts to numpy array\"\"\"\n",
    "        # Ensure all features have the same structure\n",
    "        validated_features = []\n",
    "        print(\"Converting the features to numpy array...\")\n",
    "        \n",
    "        # Define expected feature structure\n",
    "        feature_template = {\n",
    "            'semgrep_info': 0,\n",
    "            'semgrep_warning': 0,\n",
    "            'semgrep_error': 0,\n",
    "            'semgrep_total_findings': 0,\n",
    "            'semgrep_rules_triggered': 0,\n",
    "            'line_no': -1\n",
    "        }\n",
    "        \n",
    "        # Add semantic and lexical feature placeholders\n",
    "        for i in range(768):\n",
    "            feature_template[f'semantic_{i}'] = 0\n",
    "        for i in range(300):\n",
    "            feature_template[f'lexical_{i}'] = 0\n",
    "            \n",
    "        for feat in features_list:\n",
    "            if not isinstance(feat, dict):\n",
    "                validated_features.append(feature_template.copy())\n",
    "                continue\n",
    "                \n",
    "            # Create safe feature dict\n",
    "            safe_feat = feature_template.copy()\n",
    "            safe_feat.update({\n",
    "                k: v for k, v in feat.items() \n",
    "                if k in feature_template\n",
    "            })\n",
    "            \n",
    "            validated_features.append(safe_feat)\n",
    "        \n",
    "        print(\"Conversion to numpy array is done...\")\n",
    "        return self.vectorizer.fit_transform(validated_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Model Saving\n",
    "import joblib\n",
    "import pickle\n",
    "from keras.models import save_model\n",
    "import os\n",
    "\n",
    "def save_ensemble_model(ensemble, vectorizer, file_prefix='vulnerability_model'):\n",
    "    \"\"\"Save all components of the ensemble system\"\"\"\n",
    "    # Create directory if it doesn't exist\n",
    "    os.makedirs('saved_models_model3', exist_ok=True)\n",
    "    file_prefix = f'saved_models_model3/{file_prefix}'\n",
    "    \n",
    "    # 1. Save the feature vectorizer\n",
    "    joblib.dump(vectorizer.vectorizer, f'{file_prefix}_vectorizer.pkl', protocol=4)\n",
    "    \n",
    "    # 2. Save classification models\n",
    "    if hasattr(ensemble, 'classification_models'):\n",
    "        with open(f'{file_prefix}_clf_models.pkl', 'wb') as f:\n",
    "            pickle.dump(ensemble.classification_models, f)\n",
    "    \n",
    "    # 3. Save Keras models\n",
    "    if hasattr(ensemble, 'cnn_models'):\n",
    "        for i, (name, model) in enumerate(ensemble.cnn_models.items()):\n",
    "            model.save(f'{file_prefix}_cnn_{i}.h5')\n",
    "    \n",
    "    # 4. Save full ensemble\n",
    "    with open(f'{file_prefix}_full_ensemble.pkl', 'wb') as f:\n",
    "        pickle.dump(ensemble, f)\n",
    "    \n",
    "    print(f\"Models saved to directory: saved_models/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. Vulnerability Predictor\n",
    "class VulnerabilityPredictor:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.data_loader = VulnerabilityDataLoader(config)\n",
    "        self.structural_extractor = StructuralFeatureExtractor()\n",
    "        self.semantic_extractor = SemanticFeatureExtractor()\n",
    "        self.lexical_extractor = LexicalFeatureExtractor()\n",
    "        self.vectorizer = FeatureVectorizer()\n",
    "        \n",
    "    def process_language(self, language):\n",
    "        \"\"\"Process data with line number info\"\"\"\n",
    "        df = self.data_loader.load_data(language)\n",
    "        features = []\n",
    "        labels = []\n",
    "        \n",
    "        # Define severity mapping that works for all languages\n",
    "        severity_map = {\n",
    "            '0': 0,    # For Python's \"none\" cases\n",
    "            '1': 1,    # Low severity\n",
    "            '2': 2,    # Medium severity\n",
    "            '3': 3,    # High severity\n",
    "            'low': 1,\n",
    "            'medium': 2,\n",
    "            'high': 3,\n",
    "            'none': 0,\n",
    "            'info': 0,\n",
    "            '': 0      # Default for missing/empty values\n",
    "        }\n",
    "        print(f\"Processing {language} data...\")\n",
    "        \n",
    "        for _, row in df.iterrows():\n",
    "            code = self.data_loader.get_source_code(row['file_path'], language)\n",
    "            if code is None:\n",
    "                continue\n",
    "                \n",
    "            # Extract features with attention\n",
    "            structural = self.structural_extractor.extract_features(code, language)\n",
    "            \n",
    "            # Get important lines from attention\n",
    "            line_no = structural.get('line_no', -1)\n",
    "            \n",
    "            features.append({\n",
    "                'structural': structural,\n",
    "                'semantic': self.semantic_extractor.extract_features(code),\n",
    "                'lexical': self.lexical_extractor.extract_features(code),\n",
    "                'line_no': line_no\n",
    "            })\n",
    "            \n",
    "            # Standardize severity labels across languages\n",
    "            severity = str(row['severity']).lower()\n",
    "            labels.append(severity_map.get(severity, 0))  # Default to 0 if unknown\n",
    "            \n",
    "        return features, labels\n",
    "    \n",
    "    def run(self):\n",
    "        \"\"\"Run the complete vulnerability prediction pipeline\"\"\"\n",
    "        # Process all languages\n",
    "        languages = ['Python', 'CandCpp', 'CSharp', 'Java']\n",
    "        \n",
    "        all_features = []\n",
    "        all_labels = []\n",
    "        \n",
    "        for lang in languages:\n",
    "            features, labels = self.process_language(lang)\n",
    "            all_features.extend(features)\n",
    "            all_labels.extend(labels)\n",
    "            \n",
    "        # Convert features to numpy array\n",
    "        X = self.vectorizer.fit_transform(all_features)\n",
    "        y = np.array(all_labels)\n",
    "        \n",
    "        # Evaluate models\n",
    "        evaluator = Evaluator(X, y)\n",
    "        results = evaluator.evaluate_models()\n",
    "\n",
    "        if hasattr(evaluator, 'ensemble') and evaluator.ensemble is not None:\n",
    "            save_ensemble_model(evaluator.ensemble, self.vectorizer)\n",
    "        else:\n",
    "            print(\"Warning: No ensemble model to save\")\n",
    "            \n",
    "        # Print results\n",
    "        print(\"\\nModel Evaluation Results:\")\n",
    "        for model_name, metrics in results.items():\n",
    "            print(f\"\\n{model_name}:\")\n",
    "            for metric_name, value in metrics.items():\n",
    "                print(f\"{metric_name}: {value:.4f}\")\n",
    "                \n",
    "        return evaluator, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. Configuration\n",
    "config = {\n",
    "    'paths': { \n",
    "        'sample_CandCpp.csv': '/Users/shreyanandini/Desktop/MAJOR/Datasets/sample_CandCpp.csv',\n",
    "        'sample_CSharp.csv': '/Users/shreyanandini/Desktop/MAJOR/Datasets/sample_CSharp.csv',\n",
    "        'sample_Java.csv': '/Users/shreyanandini/Desktop/MAJOR/Datasets/sample_Java.csv',\n",
    "        'sample_Python.csv': '/Users/shreyanandini/Desktop/MAJOR/Datasets/sample_Python.csv'\n",
    "    },\n",
    "    'source_code_paths': {\n",
    "        'CandCppSourceCodes': '/Users/shreyanandini/Desktop/MAJOR/Datasets/CandCppSourceCodes',\n",
    "        'CSharpSourceCodes': '/Users/shreyanandini/Desktop/MAJOR/Datasets/CSharpSourceCodes',\n",
    "        'JavaSourceCodes': '/Users/shreyanandini/Desktop/MAJOR/Datasets/JavaSourceCodes'\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data for language: Python\n",
      "Processing Python data...\n",
      "Loaded data for language: CandCpp\n",
      "Processing CandCpp data...\n",
      "Loaded data for language: CSharp\n",
      "Processing CSharp data...\n",
      "Loaded data for language: Java\n",
      "Processing Java data...\n",
      "Converting the features to numpy array...\n",
      "Conversion to numpy array is done...\n",
      "Training set counts:  Counter({2: 38, 1: 38, 3: 37, 0: 2})\n",
      "Testing set counts:  Counter({1: 10, 2: 9, 3: 9, 0: 1})\n",
      "Training Classification models with hyperparameter tuning...\n",
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
      "Best params for random_forest: {'max_depth': None, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
      "Best params for xgboost: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100}\n",
      "Training classification models completed...\n",
      "Training classification models completed...\n",
      "Input shape for CNN models: (115, 1074)\n",
      "\n",
      "Training attention_model...\n",
      "Epoch 1/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 91ms/step - accuracy: 0.2913 - loss: 1.5706 - val_accuracy: 0.6087 - val_loss: 0.9334\n",
      "Epoch 2/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.3888 - loss: 1.5720 - val_accuracy: 0.6087 - val_loss: 1.1551\n",
      "Epoch 3/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.4151 - loss: 1.4733 - val_accuracy: 0.2609 - val_loss: 1.0958\n",
      "Epoch 4/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.3037 - loss: 1.4833 - val_accuracy: 0.6087 - val_loss: 0.9879\n",
      "Epoch 5/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.3551 - loss: 1.4094 - val_accuracy: 0.6087 - val_loss: 0.9844\n",
      "Epoch 6/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.3225 - loss: 1.3685 - val_accuracy: 0.5652 - val_loss: 0.9938\n",
      "\n",
      "Training graphsage_model...\n",
      "Epoch 1/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.2335 - loss: 1.3864 - val_accuracy: 0.5217 - val_loss: 1.3453\n",
      "Epoch 2/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.4151 - loss: 1.3482 - val_accuracy: 0.6087 - val_loss: 1.3046\n",
      "Epoch 3/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.3254 - loss: 1.3167 - val_accuracy: 0.3043 - val_loss: 1.2581\n",
      "Epoch 4/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.3901 - loss: 1.2763 - val_accuracy: 0.3913 - val_loss: 1.2090\n",
      "Epoch 5/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.4307 - loss: 1.2416 - val_accuracy: 0.6087 - val_loss: 1.1647\n",
      "Epoch 6/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.3847 - loss: 1.2353 - val_accuracy: 0.6087 - val_loss: 1.1261\n",
      "Epoch 7/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.5173 - loss: 1.1760 - val_accuracy: 0.6087 - val_loss: 1.0913\n",
      "Epoch 8/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.4815 - loss: 1.1593 - val_accuracy: 0.6087 - val_loss: 1.0724\n",
      "Epoch 9/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.4696 - loss: 1.2189 - val_accuracy: 0.6522 - val_loss: 1.0656\n",
      "Epoch 10/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.4509 - loss: 1.2242 - val_accuracy: 0.6522 - val_loss: 1.0603\n",
      "Epoch 11/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.3808 - loss: 1.1475 - val_accuracy: 0.6087 - val_loss: 1.0543\n",
      "Epoch 12/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.3629 - loss: 1.2254 - val_accuracy: 0.6087 - val_loss: 1.0579\n",
      "Epoch 13/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.4229 - loss: 1.1542 - val_accuracy: 0.6087 - val_loss: 1.0562\n",
      "Epoch 14/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.3940 - loss: 1.1742 - val_accuracy: 0.6087 - val_loss: 1.0584\n",
      "Epoch 15/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.4136 - loss: 1.1499 - val_accuracy: 0.5652 - val_loss: 1.0617\n",
      "Epoch 16/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.4744 - loss: 1.1444 - val_accuracy: 0.5217 - val_loss: 1.0658\n",
      "\n",
      "Training simple_gnn...\n",
      "Epoch 1/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.3247 - loss: 1.3743 - val_accuracy: 0.5217 - val_loss: 1.3365\n",
      "Epoch 2/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.3604 - loss: 1.3420 - val_accuracy: 0.5217 - val_loss: 1.2886\n",
      "Epoch 3/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.4096 - loss: 1.3007 - val_accuracy: 0.5217 - val_loss: 1.2338\n",
      "Epoch 4/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.3995 - loss: 1.2628 - val_accuracy: 0.5217 - val_loss: 1.1798\n",
      "Epoch 5/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.4604 - loss: 1.2181 - val_accuracy: 0.6087 - val_loss: 1.1342\n",
      "Epoch 6/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.4010 - loss: 1.1991 - val_accuracy: 0.6087 - val_loss: 1.1001\n",
      "Epoch 7/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.3714 - loss: 1.1921 - val_accuracy: 0.6522 - val_loss: 1.0777\n",
      "Epoch 8/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.3956 - loss: 1.2167 - val_accuracy: 0.5652 - val_loss: 1.0669\n",
      "Epoch 9/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.3949 - loss: 1.1846 - val_accuracy: 0.5652 - val_loss: 1.0605\n",
      "Epoch 10/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.3949 - loss: 1.1709 - val_accuracy: 0.5652 - val_loss: 1.0590\n",
      "Epoch 11/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.4416 - loss: 1.1472 - val_accuracy: 0.6087 - val_loss: 1.0610\n",
      "Epoch 12/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.3675 - loss: 1.1441 - val_accuracy: 0.6087 - val_loss: 1.0675\n",
      "Epoch 13/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.4331 - loss: 1.1976 - val_accuracy: 0.5217 - val_loss: 1.0746\n",
      "Epoch 14/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.4564 - loss: 1.1708 - val_accuracy: 0.5652 - val_loss: 1.0837\n",
      "Epoch 15/20\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.3949 - loss: 1.1586 - val_accuracy: 0.5652 - val_loss: 1.0848\n",
      "Training Graph CNN models completed...\n",
      "Creating Ensemble model...\n",
      "Ensembling Model Done...\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got Results...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models saved to directory: saved_models/\n",
      "\n",
      "Model Evaluation Results:\n",
      "\n",
      "random_forest:\n",
      "accuracy: 0.6897\n",
      "precision: 0.6777\n",
      "recall: 0.6897\n",
      "f1: 0.6780\n",
      "\n",
      "knn:\n",
      "accuracy: 0.6552\n",
      "precision: 0.6968\n",
      "recall: 0.6552\n",
      "f1: 0.6400\n",
      "\n",
      "svm:\n",
      "accuracy: 0.5517\n",
      "precision: 0.5862\n",
      "recall: 0.5517\n",
      "f1: 0.5188\n",
      "\n",
      "naive_bayes:\n",
      "accuracy: 0.4483\n",
      "precision: 0.4897\n",
      "recall: 0.4483\n",
      "f1: 0.4103\n",
      "\n",
      "xgboost:\n",
      "accuracy: 0.6897\n",
      "precision: 0.6777\n",
      "recall: 0.6897\n",
      "f1: 0.6780\n",
      "\n",
      "attention_model:\n",
      "accuracy: 0.3103\n",
      "precision: 0.0963\n",
      "recall: 0.3103\n",
      "f1: 0.1470\n",
      "\n",
      "graphsage_model:\n",
      "accuracy: 0.3448\n",
      "precision: 0.1189\n",
      "recall: 0.3448\n",
      "f1: 0.1768\n",
      "\n",
      "simple_gnn:\n",
      "accuracy: 0.3448\n",
      "precision: 0.1189\n",
      "recall: 0.3448\n",
      "f1: 0.1768\n",
      "\n",
      "ensemble:\n",
      "accuracy: 0.6207\n",
      "precision: 0.6054\n",
      "recall: 0.6207\n",
      "f1: 0.6075\n"
     ]
    }
   ],
   "source": [
    "# 13. Main Execution\n",
    "if __name__ == \"__main__\":\n",
    "    predictor = VulnerabilityPredictor(config)\n",
    "    evaluator, results = predictor.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def plot_model_metrics_comparison(results):\n",
    "    \"\"\"\n",
    "    Plot bar charts comparing all models' performance metrics side by side\n",
    "    \"\"\"\n",
    "    model_names = list(results.keys())\n",
    "    metrics = ['accuracy', 'precision', 'recall', 'f1']\n",
    "    \n",
    "    for metric in metrics:\n",
    "        values = [results[model][metric] for model in model_names]\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        bars = plt.bar(model_names, values, color=plt.cm.tab20(np.linspace(0, 1, len(model_names))))\n",
    "        plt.title(f'{metric.capitalize()} Comparison Across Models', fontsize=14)\n",
    "        plt.ylim(0, 1.05)\n",
    "        plt.xticks(rotation=45)\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            plt.text(bar.get_x() + bar.get_width()/2., height, f'{height:.3f}', ha='center', va='bottom')\n",
    "        plt.ylabel('Score')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "def plot_individual_model_metrics(results):\n",
    "    \"\"\"\n",
    "    Create bar charts for each model showing all metrics\n",
    "    \"\"\"\n",
    "    metrics = ['accuracy', 'precision', 'recall', 'f1']\n",
    "    for model_name, metrics_dict in results.items():\n",
    "        values = [metrics_dict[m] for m in metrics]\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        bars = plt.bar(metrics, values, color='cornflowerblue')\n",
    "        plt.title(f'Metrics for {model_name}', fontsize=14)\n",
    "        plt.ylim(0, 1.05)\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            plt.text(bar.get_x() + bar.get_width()/2., height, f'{height:.3f}', ha='center', va='bottom')\n",
    "        plt.ylabel('Score')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "def plot_model_metrics_trend(results):\n",
    "    \"\"\"\n",
    "    Plot line chart showing the trend of each metric across models\n",
    "    \"\"\"\n",
    "    model_names = list(results.keys())\n",
    "    metrics = ['accuracy', 'precision', 'recall', 'f1']\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for metric in metrics:\n",
    "        values = [results[model][metric] for model in model_names]\n",
    "        plt.plot(model_names, values, marker='o', label=metric)\n",
    "    plt.title('Performance Metrics Across Models')\n",
    "    plt.xlabel('Models')\n",
    "    plt.ylabel('Score')\n",
    "    plt.ylim(0, 1.05)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_confusion_matrices(evaluator, results):\n",
    "    \"\"\"\n",
    "    Plot confusion matrices for all models that support it\n",
    "    \"\"\"\n",
    "    if not hasattr(evaluator, 'X_test') or not hasattr(evaluator, 'y_test'):\n",
    "        print(\"Test data not available in evaluator\")\n",
    "        return\n",
    "\n",
    "    clf_models = {name: model for name, model in evaluator.ensemble.classification_models.items() \n",
    "                  if hasattr(model, 'predict')}\n",
    "\n",
    "    for name, model in clf_models.items():\n",
    "        y_pred = model.predict(evaluator.X_test)\n",
    "        cm = confusion_matrix(evaluator.y_test, y_pred)\n",
    "        \n",
    "        plt.figure(figsize=(6, 5))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                    xticklabels=['None', 'Low', 'Medium', 'High'],\n",
    "                    yticklabels=['None', 'Low', 'Medium', 'High'])\n",
    "        plt.title(f'Confusion Matrix: {name}')\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('Actual')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "def plot_ensemble_performance(evaluator):\n",
    "    \"\"\"\n",
    "    Plot training vs test performance for the ensemble model\n",
    "    \"\"\"\n",
    "    if not hasattr(evaluator, 'ensemble'):\n",
    "        print(\"No ensemble model found in evaluator\")\n",
    "        return\n",
    "\n",
    "    y_train_pred = evaluator.ensemble.predict(evaluator.X_train)\n",
    "    y_test_pred = evaluator.ensemble.predict(evaluator.X_test)\n",
    "\n",
    "    train_metrics = {\n",
    "        'accuracy': accuracy_score(evaluator.y_train, y_train_pred),\n",
    "        'precision': precision_score(evaluator.y_train, y_train_pred, average='weighted'),\n",
    "        'recall': recall_score(evaluator.y_train, y_train_pred, average='weighted'),\n",
    "        'f1': f1_score(evaluator.y_train, y_train_pred, average='weighted')\n",
    "    }\n",
    "\n",
    "    test_metrics = {\n",
    "        'accuracy': accuracy_score(evaluator.y_test, y_test_pred),\n",
    "        'precision': precision_score(evaluator.y_test, y_test_pred, average='weighted'),\n",
    "        'recall': recall_score(evaluator.y_test, y_test_pred, average='weighted'),\n",
    "        'f1': f1_score(evaluator.y_test, y_test_pred, average='weighted')\n",
    "    }\n",
    "\n",
    "    metrics = ['accuracy', 'precision', 'recall', 'f1']\n",
    "    x = np.arange(len(metrics))\n",
    "    width = 0.35\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    train_bars = plt.bar(x - width/2, [train_metrics[m] for m in metrics], width, label='Train')\n",
    "    test_bars = plt.bar(x + width/2, [test_metrics[m] for m in metrics], width, label='Test')\n",
    "\n",
    "    plt.xticks(x, metrics)\n",
    "    plt.ylim(0, 1.05)\n",
    "    plt.title('Ensemble Model: Training vs Test Performance')\n",
    "    plt.ylabel('Score')\n",
    "    plt.legend()\n",
    "\n",
    "    for bars in [train_bars, test_bars]:\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            plt.text(bar.get_x() + bar.get_width()/2., height, f'{height:.3f}', ha='center', va='bottom')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_class_distribution(y_train, y_test):\n",
    "    \"\"\"\n",
    "    Plot the class distribution in training and test sets\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "    unique_train, counts_train = np.unique(y_train, return_counts=True)\n",
    "    axes[0].bar(unique_train, counts_train, color='skyblue')\n",
    "    axes[0].set_title('Training Set Class Distribution')\n",
    "    axes[0].set_xlabel('Severity Class')\n",
    "    axes[0].set_ylabel('Count')\n",
    "    axes[0].set_xticks(unique_train)\n",
    "    axes[0].set_xticklabels(['None', 'Low', 'Medium', 'High'])\n",
    "\n",
    "    unique_test, counts_test = np.unique(y_test, return_counts=True)\n",
    "    axes[1].bar(unique_test, counts_test, color='lightgreen')\n",
    "    axes[1].set_title('Test Set Class Distribution')\n",
    "    axes[1].set_xlabel('Severity Class')\n",
    "    axes[1].set_ylabel('Count')\n",
    "    axes[1].set_xticks(unique_test)\n",
    "    axes[1].set_xticklabels(['None', 'Low', 'Medium', 'High'])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def generate_all_visualizations(evaluator, results):\n",
    "    \"\"\"\n",
    "    Generate all visualizations for the model evaluation\n",
    "    \"\"\"\n",
    "    plot_model_metrics_comparison(results)\n",
    "    plot_individual_model_metrics(results)\n",
    "    plot_model_metrics_trend(results)\n",
    "    plot_confusion_matrices(evaluator, results)\n",
    "    plot_ensemble_performance(evaluator)\n",
    "    if hasattr(evaluator, 'y_train') and hasattr(evaluator, 'y_test'):\n",
    "        plot_class_distribution(evaluator.y_train, evaluator.y_test)\n",
    "\n",
    "        \n",
    "generate_all_visualizations(evaluator, results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venviron",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
